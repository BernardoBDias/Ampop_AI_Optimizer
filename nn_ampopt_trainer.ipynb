{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "[1.1289453e+03 1.3182567e+07 2.3636074e+07 1.9800040e-01]\n",
      "[2.5278605e-05 3.5901721e-05 4.0834824e-05 6.3069965e-05 4.7000000e+01]\n"
     ]
    }
   ],
   "source": [
    "with open('database_ampop_LossF.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # Lê a primeira linha como cabeçalho (opcional)\n",
    "    data_base = [list(map(float, row)) for row in reader]\n",
    "\n",
    "data_base = torch.tensor(data_base)\n",
    "X_medidas = data_base[:,-4:].numpy()\n",
    "Y_larguras = data_base[:,:-4].numpy()\n",
    "\n",
    "print(len(X_medidas))\n",
    "# print(Y_larguras[0,:])\n",
    "print(X_medidas[0,:])\n",
    "print(Y_larguras[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4044183  0.45701337 0.23368546 0.35392308]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "size_data_test = 0.3\n",
    "hidden_size1 = 8\n",
    "hidden_size2 = 4\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()\n",
    "\n",
    "X_normalized = scaler_X.fit_transform(X_medidas)\n",
    "Y_normalized = scaler_Y.fit_transform(Y_larguras)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_normalized, Y_normalized, test_size=size_data_test)\n",
    "print(X_normalized[0,:])\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "Y_val = torch.Tensor(Y_val)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmpOpT_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(4,hidden_size1)\n",
    "        self.h_layer1 = nn.Linear(hidden_size1,hidden_size2)\n",
    "        self.h_layer2 = nn.Linear(hidden_size2,5)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.input_layer(x))\n",
    "        out = self.relu(self.h_layer1(out))\n",
    "        out = self.h_layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000, Train Loss: 0.0434, Val Loss: 0.0454\n",
      "Epoch 200/2000, Train Loss: 0.0427, Val Loss: 0.0445\n",
      "Epoch 300/2000, Train Loss: 0.0427, Val Loss: 0.0445\n",
      "Epoch 400/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 500/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 600/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 700/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 800/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 900/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1000/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1100/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1200/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1300/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1400/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1500/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1600/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1700/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1800/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 1900/2000, Train Loss: 0.0420, Val Loss: 0.0443\n",
      "Epoch 2000/2000, Train Loss: 0.0420, Val Loss: 0.0443\n"
     ]
    }
   ],
   "source": [
    "AmpOpT_pred_model = AmpOpT_NN()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(AmpOpT_pred_model.parameters(), lr = learning_rate, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    AmpOpT_pred_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        output = AmpOpT_pred_model(X_train)\n",
    "        l = loss(output, Y_train)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += l.item()\n",
    "\n",
    "    train_loss /= len(train_loader) \n",
    "\n",
    "    AmpOpT_pred_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, Y_val_batch in val_loader:\n",
    "            val_pred = AmpOpT_pred_model(X_val_batch)\n",
    "            val_loss += loss(val_pred, Y_val_batch).item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, measures):\n",
    "    model.eval()\n",
    "    measures_normalized = scaler_X.transform([measures])\n",
    "    measures_tensor = torch.tensor(measures_normalized, dtype=torch.float32)\n",
    "    predictions = model(measures_tensor).detach().numpy()\n",
    "    return scaler_Y.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ganho DC: Max = 2789.3887 ; Min = 1.4531754\n",
      "Freq Corte: Max = 27542288.0 ; Min = 1096478.2\n",
      "Slew Rate: Max = 97769620.0 ; Min = 1029260.0\n",
      "Custo: Max = 0.43993607 ; Min = 0.06546725\n"
     ]
    }
   ],
   "source": [
    "print('Ganho DC: Max =', max(X_medidas[:,0]), '; Min =', min(X_medidas[:,0]))\n",
    "print('Freq Corte: Max =', max(X_medidas[:,1]), '; Min =', min(X_medidas[:,1]))\n",
    "print('Slew Rate: Max =', max(X_medidas[:,2]), '; Min =', min(X_medidas[:,2]))\n",
    "print('Custo: Max =', max(X_medidas[:,3]), '; Min =', min(X_medidas[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larguras de canal previstas: [[2.8612694e-05 1.5153638e-05 3.4545010e-05 2.2851353e-05 2.2770475e+01]]\n"
     ]
    }
   ],
   "source": [
    "novas_medidas = [1.5e3, 8e6, 10e6, 0.1]\n",
    "larguras_previstas = predict(AmpOpT_pred_model, novas_medidas)\n",
    "print(\"Larguras de canal previstas:\", larguras_previstas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
